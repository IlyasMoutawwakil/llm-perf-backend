name: NVIDIA benchmarks

on:
  workflow_dispatch:
  schedule:
    - cron: "0 */6 * * *"

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: false

jobs:
  build_image_and_run_benchmarks:
    strategy:
      matrix:
        runner: [hf-dgx-01]

    runs-on: ${{ matrix.runner }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Build image
        run: docker build
          --file docker/cuda.dockerfile
          --build-arg USER_ID=$(id -u)
          --build-arg GROUP_ID=$(id -g)
          --build-arg CUDA_VERSION=11.8.0
          --build-arg TORCH_CUDA=cu118
          --tag llm-perf-cuda:11.8.0
          .

      - name: Run tests
        run: docker run
          --rm
          --gpus all
          --net host
          --pid host
          --env USE_CUDA=1
          --env HF_TOKEN=$HF_TOKEN
          --entrypoint /bin/bash
          --volume $(pwd):/workspace/llm-perf
          --volume $HOME/.cache/huggingface:/home/user/.cache/huggingface
          --workdir /workspace/llm-perf
          llm-perf-cuda:11.8.0
          -c "pip install -r requirements/cuda.txt && pip install flash-attn --no-build-isolation ;

          python scripts/pull_dataset.py &&
          python scripts/benchmark.py --config pytorch+cuda+float16 &&
          python scripts/benchmark.py --config pytorch+cuda+float16+bnb-4bit &&
          python scripts/benchmark.py --config pytorch+cuda+float16+bnb-8bit &&
          python scripts/benchmark.py --config pytorch+cuda+float16+bettertransformer &&
          python scripts/benchmark.py --config pytorch+cuda+float16+flash-attention-v2 &&
          python scripts/benchmark.py --config pytorch+cuda+float16+gptq-4bit+cuda-fp16 &&
          python scripts/benchmark.py --config pytorch+cuda+float16+gptq-4bit+exllama-v1 &&
          python scripts/benchmark.py --config pytorch+cuda+float16+gptq-4bit+exllama-v2 &&
          python scripts/benchmark.py --config pytorch+cuda+float16+bnb-4bit+bettertransformer &&
          python scripts/benchmark.py --config pytorch+cuda+float16+bnb-8bit+bettertransformer &&
          python scripts/benchmark.py --config pytorch+cuda+float16+bnb-4bit+flash-attention-v2 &&
          python scripts/benchmark.py --config pytorch+cuda+float16+bnb-8bit+flash-attention-v2 &&
          python scripts/push_dataset.py

          echo 'Done'"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}

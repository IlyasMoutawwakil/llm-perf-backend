name: Benchmarks on the DGX

on:
  workflow_dispatch:
  schedule:
    - cron: "0 0 * * *"

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    runs-on: hf-dgx-01
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Build image
        run: docker build
          --file docker/cuda.dockerfile
          --build-arg USER_ID=$(id -u)
          --build-arg GROUP_ID=$(id -g)
          --build-arg CUDA_VERSION=11.8.0
          --build-arg TORCH_CUDA=cu118
          --tag llm-perf-cuda:11.8.0
          .

      - name: Run tests
        run: docker run
          --rm
          --net host
          --pid host
          --env USE_CUDA=1
          --env MACHINE=hf-dgx-01
          --env HF_TOKEN=$HF_TOKEN
          --gpus '"device=4"'
          --entrypoint /bin/bash
          --volume $(pwd):/workspace/llm-perf
          --volume $HOME/.cache/huggingface:/home/user/.cache/huggingface
          --workdir /workspace/llm-perf
          llm-perf-cuda:11.8.0
          -c "pip install -r requirements/cuda.txt ;
          pip install packaging && pip install flash-attn --no-build-isolation ;

          python src/pull_dataset.py ;

          python src/benchmark.py --config pytorch+cuda+float16 ;
          python src/benchmark.py --config pytorch+cuda+float32 ;
          python src/benchmark.py --config pytorch+cuda+float16+bnb-4bit ;
          python src/benchmark.py --config pytorch+cuda+float16+bettertransformer ;
          python src/benchmark.py --config pytorch+cuda+float16+flash-attention-v2 ;
          python src/benchmark.py --config pytorch+cuda+float16+gptq-4bit+exllama-v1 ;
          python src/benchmark.py --config pytorch+cuda+float16+gptq-4bit+exllama-v2 ;
          python src/benchmark.py --config pytorch+cuda+float16+bnb-4bit+bettertransformer ;
          python src/benchmark.py --config pytorch+cuda+float16+bnb-4bit+flash-attention-v2 ;

          python src/push_dataset.py ;"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
